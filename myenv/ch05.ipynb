{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:49.841983Z",
     "start_time": "2024-07-16T07:30:49.838903Z"
    }
   },
   "source": [
    "# 계층 단위로 구현하자\n",
    "# 계층이란 -> 기능의 최소 단위로 덧셈, 곱셈, 활성화 함수, 행렬 곱 의미\n",
    "\n",
    "class MulLayer: # 곱셈 노드\n",
    "    def __init__(self): # 초기화 함수 => forward Pass인 경우, 입력값을 유지하기 위해서 = Input 저장 변수\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y): # 순전파 함수\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): # 역전파 함수\n",
    "        dx = dout * self.y # 서로의 값을 바꾼 것에 상류에서 구한 미분값 곱하기\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "43526b6cca3bd8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:50.944609Z",
     "start_time": "2024-07-16T07:30:50.941835Z"
    }
   },
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b32b2dae17f3ae85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:51.732831Z",
     "start_time": "2024-07-16T07:30:51.730066Z"
    }
   },
   "source": [
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)\n",
    "# backward, forward 이름에서 알 수 있듯이 호출 순서가 반대임을 유의하기"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5b8651bc38c1f219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:52.131841Z",
     "start_time": "2024-07-16T07:30:52.129982Z"
    }
   },
   "source": [
    "# 덧셈 노드 구현하기\n",
    "\n",
    "class AddLayer:\n",
    "    def __init___(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):# 덧셈 노드에서는 두 Input을 더해서 보냄\n",
    "        out = x + y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): # 상류에서 내려온 미분 값(dout)을 그대로 보냄\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        \n",
    "        return dx, dy"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c1c954c1b47a08f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:52.462907Z",
     "start_time": "2024-07-16T07:30:52.459047Z"
    }
   },
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 각각 필요한 계층 불러옴\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파\n",
    "\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8d37b61ce05331bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:52.730058Z",
     "start_time": "2024-07-16T07:30:52.726846Z"
    }
   },
   "source": [
    "# 활성화 함수와 역전파, 순전파에 대한 구현\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    # 순전파 부분 구현\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) # 0 이하인 경우 True(= 1), 0 초과인 경우 False(= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # out[1] = 0, out[0] = 1\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): # 순전파에서 사용한 mask를 활용\n",
    "        dout[self.mask] = 0 # 0 이하인 경우 미분값 = 0 -> dout[1] = 0 / 0 초과인 경우 False -> dout[0] = 이전 상류 미분 값을 그대로\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6c6ed22e5117cbc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:53.149728Z",
     "start_time": "2024-07-16T07:30:53.046054Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "4a80d7393b3d22f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:53.341211Z",
     "start_time": "2024-07-16T07:30:53.339239Z"
    }
   },
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)\n",
    "# 0 보다 작아야 True, 0 보다 커야 False"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "115da7068dcddf80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:53.695197Z",
     "start_time": "2024-07-16T07:30:53.692909Z"
    }
   },
   "source": [
    "# 시그모이드 함수\n",
    "\n",
    "class Sigmoid: \n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2b05b19a7ad4cb74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:54.083718Z",
     "start_time": "2024-07-16T07:30:54.079308Z"
    }
   },
   "source": [
    "X = np.random.randn(2) # 0 혹은 1 \n",
    "W = np.random.randn(2, 3)  # 가중치\n",
    "B = np.random.randn(3) # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3b412136d3b173e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:54.413974Z",
     "start_time": "2024-07-16T07:30:54.410773Z"
    }
   },
   "source": [
    "# 신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서 어파인 변환이라고 함.\n",
    "X_dot_W = np.array([[0,0,0], [10, 10, 10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W)\n",
    "\n",
    "print(X_dot_W + B)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "cee0058c592dd02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:54.751394Z",
     "start_time": "2024-07-16T07:30:54.748223Z"
    }
   },
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis = 0)\n",
    "print(dB)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "c3fbbdf36b6d9a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:55.092664Z",
     "start_time": "2024-07-16T07:30:55.089538Z"
    }
   },
   "source": [
    "# 어파인 변환 -> 오차역전파를 단순히 스칼라 값으로 수행하는 것이 아닌 행렬을 통해서 수행하고자 함.\n",
    "# 이때,어파인의 의미는 행렬의 곱을 의미함.\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        return dx"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "11fdeb10e3655595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:55.457186Z",
     "start_time": "2024-07-16T07:30:55.455272Z"
    }
   },
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "6d06b2e11773f0b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:55.839208Z",
     "start_time": "2024-07-16T07:30:55.837537Z"
    }
   },
   "source": [
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x - c)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    y = exp_x / sum_exp_x\n",
    "    return y"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "98bc31a2e25cdfb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:57.040010Z",
     "start_time": "2024-07-16T07:30:57.036356Z"
    }
   },
   "source": [
    "# 소프트맥스 함수 & 교차 엔트로피 오차(cross entropy error)을 통한 오차역전파 확인\n",
    "# 소프트맥스 함수 & 교차 엔트로피 오차 // 항등 함수 & 오차제곱합(Sum Square Error)을 묶어서 사용 -> 오차 역전파가 차분으로 깔끔하게 계산\n",
    "# 추론 과정에선 소프트맥스, 항등함수 안 씀 -> 우리가 관심있는 건 Affine의 output(= Score)가 가장 큰 것을 찾는 것이 중요함\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "         batch_size = self.t.shape[0]\n",
    "         dx = (self.y - self.t) / batch_size\n",
    "         \n",
    "         return dx"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "897d267c892e38d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:58.381875Z",
     "start_time": "2024-07-16T07:30:58.373317Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Affine, Relu, and SoftmaxWithLoss classes should be defined or imported here\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)  # f(x + h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)  # f(x - h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val  # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 초기화 함수로 각 인수는 순서대로 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 1번째 층의 가중치\n",
    "        self.params['b1'] = np.zeros(hidden_size)# 1번째 층의 편향\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)# 2번째 층의 가중치\n",
    "        self.params['b2'] = np.zeros(output_size)# 2번째 층의 편향\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict() # 순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관\n",
    "        # 어파인 변환 - ReLu - 어파인 변환 순으로 계층 구조\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss() # 신경망의 마지막 계층으로 해당 실습에서는 SoftmaxWithLoss 활용함.\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 예측(추론)을 수행함.\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        # 정확도를 구한다.\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: \n",
    "            t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        # 가중치 매개변수의 기울기를 수치 미분 방식으로 구함, 이전 4장에서 한 것과 동일\n",
    "        # x: 입력 데이터, t: 정답 레이블\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 가중치 매개변수의 기울기를 오차역전파법으로 구함.\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        # 손실 함수의 값을 구함. x: 입력 데이터, t: 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "76d8d858bed84bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:30:59.421153Z",
     "start_time": "2024-07-16T07:30:59.415638Z"
    }
   },
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = self.argmax(y, axis =  1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis = 1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout =1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "fe0aadbb79763ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:31:01.663292Z",
     "start_time": "2024-07-16T07:31:00.916294Z"
    }
   },
   "source": [
    "!pip install import_ipynb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in ./lib/python3.12/site-packages (0.1.4)\r\n",
      "Requirement already satisfied: IPython in ./lib/python3.12/site-packages (from import_ipynb) (8.26.0)\r\n",
      "Requirement already satisfied: nbformat in ./lib/python3.12/site-packages (from import_ipynb) (5.10.4)\r\n",
      "Requirement already satisfied: decorator in ./lib/python3.12/site-packages (from IPython->import_ipynb) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in ./lib/python3.12/site-packages (from IPython->import_ipynb) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in ./lib/python3.12/site-packages (from IPython->import_ipynb) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./lib/python3.12/site-packages (from IPython->import_ipynb) (3.0.47)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./lib/python3.12/site-packages (from IPython->import_ipynb) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in ./lib/python3.12/site-packages (from IPython->import_ipynb) (0.6.3)\r\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./lib/python3.12/site-packages (from IPython->import_ipynb) (5.14.3)\r\n",
      "Requirement already satisfied: pexpect>4.3 in ./lib/python3.12/site-packages (from IPython->import_ipynb) (4.9.0)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./lib/python3.12/site-packages (from nbformat->import_ipynb) (2.20.0)\r\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./lib/python3.12/site-packages (from nbformat->import_ipynb) (4.23.0)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./lib/python3.12/site-packages (from nbformat->import_ipynb) (5.7.2)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./lib/python3.12/site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.4)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.19.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.2.2)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./lib/python3.12/site-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in ./lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import_ipynb) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in ./lib/python3.12/site-packages (from stack-data->IPython->import_ipynb) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./lib/python3.12/site-packages (from stack-data->IPython->import_ipynb) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in ./lib/python3.12/site-packages (from stack-data->IPython->import_ipynb) (0.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->IPython->import_ipynb) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:31:02.968854Z",
     "start_time": "2024-07-16T07:31:02.966298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'dataset')))"
   ],
   "id": "c9654edc5578537e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 모델 구축\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # 입력을 1차원으로 변환\n",
    "    Dense(128, activation='relu'),  # 은닉층\n",
    "    Dense(10, activation='softmax')  # 출력층\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ],
   "id": "fe480fa397631841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:37:00.517737Z",
     "start_time": "2024-07-16T07:36:53.824063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TensorFlow와 Keras를 사용한 MNIST 데이터셋 로드 및 모델 훈련\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 모델 구축\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # 입력을 1차원으로 변환\n",
    "    Dense(128, activation='relu'),  # 은닉층\n",
    "    Dense(10, activation='softmax')  # 출력층\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# TwoLayerNet 클래스와 관련된 전체 코드 포함\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "# TensorFlow로 로드한 MNIST 데이터 재사용\n",
    "t_train = y_train\n",
    "t_test = y_test\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# TwoLayerNet 모델 초기화\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 배치 데이터 준비\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "# 수치 미분과 오차 역전파의 기울기 비교\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치 매개변수의 차이의 절댓값을 구하고 이를 평균한 값이 오차가 됨.\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값의 평균을 계산\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\", diff)\n"
   ],
   "id": "e42e03bbe1b715e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 565us/step - accuracy: 0.8654 - loss: 0.4720 - val_accuracy: 0.9552 - val_loss: 0.1559\n",
      "Epoch 2/5\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 549us/step - accuracy: 0.9618 - loss: 0.1306 - val_accuracy: 0.9673 - val_loss: 0.1103\n",
      "Epoch 3/5\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 544us/step - accuracy: 0.9740 - loss: 0.0874 - val_accuracy: 0.9693 - val_loss: 0.1015\n",
      "Epoch 4/5\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 534us/step - accuracy: 0.9825 - loss: 0.0599 - val_accuracy: 0.9705 - val_loss: 0.0961\n",
      "Epoch 5/5\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 545us/step - accuracy: 0.9875 - loss: 0.0455 - val_accuracy: 0.9743 - val_loss: 0.0936\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 213us/step - accuracy: 0.9691 - loss: 0.1022\n",
      "Test accuracy: 0.9732000231742859\n",
      "W1: 3.8297385317136033e-10\n",
      "b1: 2.2994719354205247e-09\n",
      "W2: 6.227503958919009e-09\n",
      "b2: 1.399209405020252e-07\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "6ad4207e1db7d5c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:57:23.832986Z",
     "start_time": "2024-07-16T07:57:09.214880Z"
    }
   },
   "source": [
    "# 오차역전파법을 사용한 학습 구현하기\n",
    "\n",
    "iters_num = 10000 # 총 반복 횟수\n",
    "train_size = x_train.shape[0] # 훈련 데이터의 크기\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1# 학습률 설정\n",
    "\n",
    "train_loss_list = [] # 훈련 데이터의 손실 값을 저장하는 리스트\n",
    "train_acc_list = [] # 각 에포크마다 훈련 정확도를 저장하는 리스트\n",
    "test_acc_list = [] # 각 에포크마다 테스트 정확도를 저장하는 리스트\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1) # 한 에포크당 반복 횟수(데이터셋을 한 번 모두 사용하는 횟수)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size) # 무작위로 훈련 데이터에서 설정한 배치 크기만큼 선정\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch) # 기울기 계산 -> 오차 역전파를 활용해서 손실 함수의 기울기를 구하는 코드\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key] # 오차역전파로 구한 기울기에 학습률을 곱하여 최적의 가중치 매개변수로 조정\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch) # 현재 미니 배치 데이터, 정답 레이블에 대한 손실 함수 계산 -> 해당 실습에서는 교차 엔트로피 오차 사용\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)# 훈련 데이터에 대한 정확도 구하기\n",
    "        test_acc = network.accuracy(x_test, t_test)# 시험 데이터에 대한 정확도 구하기\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "        \n",
    "    #  훈련 데이터와 테스트 데이터 모두 높은 정확도를 보여주고 있음."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11163333333333333 0.1106\n",
      "0.9036666666666666 0.9096\n",
      "0.9247 0.9271\n",
      "0.9377166666666666 0.9387\n",
      "0.9469666666666666 0.944\n",
      "0.9515333333333333 0.9484\n",
      "0.9571166666666666 0.9555\n",
      "0.9600833333333333 0.9575\n",
      "0.9641333333333333 0.9599\n",
      "0.9674666666666667 0.9642\n",
      "0.9697833333333333 0.9646\n",
      "0.9716833333333333 0.9665\n",
      "0.9739666666666666 0.9679\n",
      "0.97445 0.9693\n",
      "0.9764166666666667 0.9697\n",
      "0.9772666666666666 0.9707\n",
      "0.9782833333333333 0.9697\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d496d0a10b51418d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
